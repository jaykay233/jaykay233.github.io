<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jaykay233.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Zhiyuan Xu&#39;s self blog!">
<meta property="og:type" content="website">
<meta property="og:title" content="徐志远">
<meta property="og:url" content="http://jaykay233.github.io/index.html">
<meta property="og:site_name" content="徐志远">
<meta property="og:description" content="Zhiyuan Xu&#39;s self blog!">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Jay Kay">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://jaykay233.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>徐志远</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">徐志远</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">推荐算法工程师的学习日常</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://jaykay233.github.io/2020/06/13/Joint-Item-Recommendation-and-Attribute-Inference/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jay Kay">
      <meta itemprop="description" content="Zhiyuan Xu's self blog!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="徐志远">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/13/Joint-Item-Recommendation-and-Attribute-Inference/" class="post-title-link" itemprop="url">Joint Item Recommendation and Attribute Inference</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-13 15:01:05" itemprop="dateCreated datePublished" datetime="2020-06-13T15:01:05-07:00">2020-06-13</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="论文背景"><a class="markdownIt-Anchor" href="#论文背景"></a> 论文背景</h3>
<p>现实世界中很多数据特征是缺失的。所以本文希望做半监督学习来一般填充缺失值，一边做点击率的预测任务。</p>
<h3 id="模型结构"><a class="markdownIt-Anchor" href="#模型结构"></a> 模型结构</h3>
<h4 id="图学习模块"><a class="markdownIt-Anchor" href="#图学习模块"></a> 图学习模块</h4>
<h5 id="embedding-fusion-layer"><a class="markdownIt-Anchor" href="#embedding-fusion-layer"></a> Embedding Fusion Layer</h5>
<p>这里的符号比较复杂，要先搞清楚一些定义。（对我来说有点混乱）</p>
<p>$ P \in R^{d \times M} $ 用户id的embedding矩阵</p>
<p>$ Q \in R^{d \times N} $ 物品id的embedding矩阵</p>
<p>在第 $ l $ 轮迭代的时候，近似的用户和物品属性矩阵是 $ X^l \in R^{d_x \times M} $ , $ Y^l \in R^{d_y \times N} $ 。<br />
然后第 $ l $ 层的向量是由前一层更新过来的。使用时和属性向量和id embedding向量和在一起用。<br />
$ u_a^{l,0}=[p_a, x_a^l \times W_u] $<br />
$ v_i^{l,0}=[q_i, y_i^l \times W_v] $<br />
第一轮的属性向量就由加权平均填充。</p>
<h4 id="embedding-propagation-layer"><a class="markdownIt-Anchor" href="#embedding-propagation-layer"></a> Embedding propagation Layer</h4>
<img src="/2020/06/13/Joint-Item-Recommendation-and-Attribute-Inference/1.png" class="">
<img src="/2020/06/13/Joint-Item-Recommendation-and-Attribute-Inference/2.png" class="">
<h4 id="attribute-update-module"><a class="markdownIt-Anchor" href="#attribute-update-module"></a> Attribute update module</h4>
<h5 id="prediction-part"><a class="markdownIt-Anchor" href="#prediction-part"></a> Prediction Part</h5>
<h5 id="attribute-update-part"><a class="markdownIt-Anchor" href="#attribute-update-part"></a> Attribute Update Part</h5>
<img src="/2020/06/13/Joint-Item-Recommendation-and-Attribute-Inference/4.png" class="">
<h5 id="loss"><a class="markdownIt-Anchor" href="#loss"></a> Loss</h5>
<p>inference loss + recommendation loss</p>
<img src="/2020/06/13/Joint-Item-Recommendation-and-Attribute-Inference/3.png" class="">
<h3 id="实验结果"><a class="markdownIt-Anchor" href="#实验结果"></a> 实验结果</h3>
<img src="/2020/06/13/Joint-Item-Recommendation-and-Attribute-Inference/5.png" class="">
<img src="/2020/06/13/Joint-Item-Recommendation-and-Attribute-Inference/6.png" class="">
<img src="/2020/06/13/Joint-Item-Recommendation-and-Attribute-Inference/7.png" class="">
<h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3>
<p>其实我在看这篇文章的时候一直在找inference能提升效果的原因分析，但是感觉作者介绍的不是很清楚。总的来讲思路有点像EM，涉及到的问题也是工业界比较棘手的问题。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://jaykay233.github.io/2020/06/13/deliberation-network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jay Kay">
      <meta itemprop="description" content="Zhiyuan Xu's self blog!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="徐志远">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/13/deliberation-network/" class="post-title-link" itemprop="url">deliberation network</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-13 11:05:27" itemprop="dateCreated datePublished" datetime="2020-06-13T11:05:27-07:00">2020-06-13</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://jaykay233.github.io/2020/06/13/copynet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jay Kay">
      <meta itemprop="description" content="Zhiyuan Xu's self blog!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="徐志远">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/13/copynet/" class="post-title-link" itemprop="url">copynet</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-13 11:05:15" itemprop="dateCreated datePublished" datetime="2020-06-13T11:05:15-07:00">2020-06-13</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://jaykay233.github.io/2020/06/13/Controllable-Multi-Interest-Framework-for-Recommendation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jay Kay">
      <meta itemprop="description" content="Zhiyuan Xu's self blog!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="徐志远">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/13/Controllable-Multi-Interest-Framework-for-Recommendation/" class="post-title-link" itemprop="url">Controllable Multi-Interest Framework for Recommendation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-13 09:40:19" itemprop="dateCreated datePublished" datetime="2020-06-13T09:40:19-07:00">2020-06-13</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="论文背景"><a class="markdownIt-Anchor" href="#论文背景"></a> 论文背景</h3>
<p>我一直觉得论文背景其实比之后的模型结构重要的多。因为这很体现文章的价值，比如做了什么，为什么这么做。这篇文章主要涉及的是多兴趣向量的可控性问题。</p>
<h3 id="模型结构"><a class="markdownIt-Anchor" href="#模型结构"></a> 模型结构</h3>
<p>模型结构的一些部件都很常用,感觉也没什么值得多说的。</p>
<h4 id="multi-interest-extractino"><a class="markdownIt-Anchor" href="#multi-interest-extractino"></a> Multi-Interest Extractino</h4>
<h4 id="dynamic-routing"><a class="markdownIt-Anchor" href="#dynamic-routing"></a> Dynamic Routing</h4>
<h4 id="self-attentive-method"><a class="markdownIt-Anchor" href="#self-attentive-method"></a> Self Attentive Method</h4>
<h4 id="aggregation-module"><a class="markdownIt-Anchor" href="#aggregation-module"></a> Aggregation Module</h4>
<p>$ f(u,i)=max({e_i^T} v_u^{(k)}) $</p>
<p>$ 1&lt;=k&lt;=K $</p>
<p>$ Q(u,S) = \Sigma_{i \in S} f(u,i) + \lambda \Sigma_{i \in S}\Sigma_{j \in S} g(i,j) $</p>
<h3 id="评价标准"><a class="markdownIt-Anchor" href="#评价标准"></a> 评价标准</h3>
<p>Recall<br />
Hitrate<br />
NDCG</p>
<img src="/2020/06/13/Controllable-Multi-Interest-Framework-for-Recommendation/1.png" class="">  
<img src="/2020/06/13/Controllable-Multi-Interest-Framework-for-Recommendation/2.png" class="">  
<h3 id="controllable-study"><a class="markdownIt-Anchor" href="#controllable-study"></a> Controllable Study</h3>
<img src="/2020/06/13/Controllable-Multi-Interest-Framework-for-Recommendation/3.png" class="">  
<h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3>
<p>显式更改loss得以控制精度和多样性。思想很简单，效果要检验一下才知道。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://jaykay233.github.io/2020/06/12/Reinforced-Negative-Sampling-over-Knowledge-Graph-for-Recommendation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jay Kay">
      <meta itemprop="description" content="Zhiyuan Xu's self blog!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="徐志远">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/12/Reinforced-Negative-Sampling-over-Knowledge-Graph-for-Recommendation/" class="post-title-link" itemprop="url">Reinforced Negative Sampling over Knowledge Graph for Recommendation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-12 21:03:09" itemprop="dateCreated datePublished" datetime="2020-06-12T21:03:09-07:00">2020-06-12</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="论文背景"><a class="markdownIt-Anchor" href="#论文背景"></a> 论文背景</h3>
<p>作者针对过往的负采样技术进行总结,提出了新的框架KGPolicy。一般有两种负采样，静态负采样和自适应负采样。</p>
<h4 id="静态采样"><a class="markdownIt-Anchor" href="#静态采样"></a> 静态采样</h4>
<p>静态采样一般选取均匀分布或者按照热度（流行度）负采样。缺点是采样的负样本独立于模型，对模型造成不了什么影响。</p>
<h4 id="自适应负采样"><a class="markdownIt-Anchor" href="#自适应负采样"></a> 自适应负采样</h4>
<p>自适应负采样会更加关注难样本。因为这些样本会给模型带来更大的价值。但是这些假设都基于历史。被选取的样本可能在未来出现，所以会损害模型性能。</p>
<h4 id="近期方法"><a class="markdownIt-Anchor" href="#近期方法"></a> 近期方法</h4>
<p>近期方法一般都选取综合的评价，比如展现未点击，点击未转化来加强负采样的有效程度。但依然需要更有效的措施来进行负采样。</p>
<h3 id="模型结构"><a class="markdownIt-Anchor" href="#模型结构"></a> 模型结构</h3>
<p>一般建模的目标都是</p>
<p>$ y_{ui}=f_R(u,i)=r_u^Tr_i $，</p>
<p>根据过去的一些研究，这篇文章使用了BPR损失函数。</p>
<img src="/2020/06/12/Reinforced-Negative-Sampling-over-Knowledge-Graph-for-Recommendation/1.png" class="">
<p>然后这里还涉及到负样本的信息性，衡量标准是梯度的幅值。$ \nabla_{u,i,j} = 1-\sigma(f_R(u,i)-f_R(u,j)) $。所以好的负样本要使得这个数值尽量大。</p>
<h4 id="负样本选取规则"><a class="markdownIt-Anchor" href="#负样本选取规则"></a> 负样本选取规则</h4>
<p>首先定义原子路径：<br />
$ i \rightarrow e^{`} \rightarrow j $<br />
其中 $ i $ 是和 $ u $ 发生过交互关系的，$ j $ 是路径和还没和 $ u $ 发生过交互关系的。<br />
这样做有两点好处：</p>
<ol>
<li>因为 $ i $ 和 $ j $ 都依赖于同样的实体，那么其实它们更加可能相近，所以能提供的负样本信息性更大。</li>
<li>可以反映出用户的真实兴趣。说明有足够理由相信用户确实不太对$ j $感兴趣。同样可以继续做路径扩展增加负样本的置信度。</li>
</ol>
<h4 id="强化学习"><a class="markdownIt-Anchor" href="#强化学习"></a> 强化学习</h4>
<p>强化学习涉及到几个关键问题：</p>
<ol>
<li>动作定义：<br />
$ a_t = (e_t \rightarrow e_t^{`} \rightarrow e_{t+1} ) $</li>
<li>状态动态转移：<br />
$ P(s_{t+1} = (u,e_{t+1}) | s_t = (u,e_t),a_t = (e_t \rightarrow e_t^{`} \rightarrow e_{t+1}) )=1 $</li>
<li>奖励函数：<br />
预测奖励：$ f_R(u,e_t) <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">，</mi><mi mathvariant="normal">考</mi><mi mathvariant="normal">虑</mi><mi mathvariant="normal">预</mi><mi mathvariant="normal">测</mi><mi mathvariant="normal">奖</mi><mi mathvariant="normal">励</mi><mi mathvariant="normal">是</mi><mi mathvariant="normal">为</mi><mi mathvariant="normal">了</mi><mi mathvariant="normal">获</mi><mi mathvariant="normal">得</mi><mi mathvariant="normal">更</mi><mi mathvariant="normal">大</mi><mi mathvariant="normal">信</mi><mi mathvariant="normal">息</mi><mi mathvariant="normal">度</mi><mi mathvariant="normal">的</mi><mi mathvariant="normal">样</mi><mi mathvariant="normal">本</mi><mi mathvariant="normal">。</mi><mi mathvariant="normal">相</mi><mi mathvariant="normal">似</mi><mi mathvariant="normal">奖</mi><mi mathvariant="normal">励</mi><mi mathvariant="normal">：</mi></mrow><annotation encoding="application/x-tex">，考虑预测奖励是为了获得更大信息度的样本。
相似奖励：</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">考</span><span class="mord cjk_fallback">虑</span><span class="mord cjk_fallback">预</span><span class="mord cjk_fallback">测</span><span class="mord cjk_fallback">奖</span><span class="mord cjk_fallback">励</span><span class="mord cjk_fallback">是</span><span class="mord cjk_fallback">为</span><span class="mord cjk_fallback">了</span><span class="mord cjk_fallback">获</span><span class="mord cjk_fallback">得</span><span class="mord cjk_fallback">更</span><span class="mord cjk_fallback">大</span><span class="mord cjk_fallback">信</span><span class="mord cjk_fallback">息</span><span class="mord cjk_fallback">度</span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">样</span><span class="mord cjk_fallback">本</span><span class="mord cjk_fallback">。</span><span class="mord cjk_fallback">相</span><span class="mord cjk_fallback">似</span><span class="mord cjk_fallback">奖</span><span class="mord cjk_fallback">励</span><span class="mord cjk_fallback">：</span></span></span></span> g_R(i,e_t) $，考虑相似奖励是为了获得反映用户真实兴趣爱好的样本，表明用户确实不喜欢。<br />
最后总的奖励 = 相似奖励 + 预测奖励</li>
<li>目标函数：</li>
</ol>
<img src="/2020/06/12/Reinforced-Negative-Sampling-over-Knowledge-Graph-for-Recommendation/2.png" class="">
<h4 id="知识图策略网"><a class="markdownIt-Anchor" href="#知识图策略网"></a> 知识图策略网</h4>
<h5 id="graphsage获取节点特征"><a class="markdownIt-Anchor" href="#graphsage获取节点特征"></a> GraphSage获取节点特征</h5>
<h5 id="邻居注意力模块"><a class="markdownIt-Anchor" href="#邻居注意力模块"></a> 邻居注意力模块</h5>
<p>$ P(a_t|s_t) = P((e_t,e_t^{`})|s_t) * P((e_t^{`},e_{t+1}) | s_t, (e_t,e_t^{`})) $<br />
这个模块由两部分注意力模块构成，分别是（1）知识图谱的邻居注意力和（2）物品的邻居注意力<br />
（1）<img src="/2020/06/12/Reinforced-Negative-Sampling-over-Knowledge-Graph-for-Recommendation/3.png" class=""></p>
<p>（2）<img src="/2020/06/12/Reinforced-Negative-Sampling-over-Knowledge-Graph-for-Recommendation/4.png" class=""></p>
<h5 id="邻居剪枝"><a class="markdownIt-Anchor" href="#邻居剪枝"></a> 邻居剪枝</h5>
<p>为了减少不必要的探索并且保证样本的效果，提出了一个新的策略，具体是</p>
<ol>
<li>先从节点的邻居降采样或者过采样得到一个集合的子集</li>
<li>然后从全空间负采样得到另外一些节点保证多样性</li>
<li>根据内积打分函数选取与节点最相似的几个节点作为返回结果<br />
这样既降低了时间复杂度也保证了效果</li>
</ol>
<h3 id="优化"><a class="markdownIt-Anchor" href="#优化"></a> 优化</h3>
<h4 id="推荐器的优化"><a class="markdownIt-Anchor" href="#推荐器的优化"></a> 推荐器的优化</h4>
<p>固定采样器参数，优化推荐器</p>
<h4 id="采样器的优化"><a class="markdownIt-Anchor" href="#采样器的优化"></a> 采样器的优化</h4>
<p>老套路，上reinforce算法做策略梯度</p>
<h3 id="假阴性问题"><a class="markdownIt-Anchor" href="#假阴性问题"></a> 假阴性问题</h3>
<p>论文中就说知识图谱的鲁棒性更加高一点，虽然这个问题确实无法避免。</p>
<h3 id="kgpolicy实验"><a class="markdownIt-Anchor" href="#kgpolicy实验"></a> KGPolicy实验</h3>
<p>实验为了说明几个问题</p>
<ol>
<li>和现在的主流方法们比结果怎么样</li>
</ol>
<img src="/2020/06/12/Reinforced-Negative-Sampling-over-Knowledge-Graph-for-Recommendation/5.png" class="">
<ol start="2">
<li>KGPolicy的参数影响</li>
</ol>
<img src="/2020/06/12/Reinforced-Negative-Sampling-over-Knowledge-Graph-for-Recommendation/6.png" class="">
<ol start="3">
<li>负样本的深层次分析</li>
</ol>
<img src="/2020/06/12/Reinforced-Negative-Sampling-over-Knowledge-Graph-for-Recommendation/7.png" class="">
<h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3>
<p>这篇文章其实还是有点东西的。读一读类似的文章对自己还是会有帮助的。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://jaykay233.github.io/2020/06/11/Neural-Graph-Collaborate-Filtering/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jay Kay">
      <meta itemprop="description" content="Zhiyuan Xu's self blog!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="徐志远">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/11/Neural-Graph-Collaborate-Filtering/" class="post-title-link" itemprop="url">Neural Graph Collaborate Filtering</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-11 16:57:41" itemprop="dateCreated datePublished" datetime="2020-06-11T16:57:41-07:00">2020-06-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">推荐系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>接下来一段时间可能会主要关注图学习在推荐系统中的运用。先来一篇何老师的NGCF。</p>
<h3 id="论文背景"><a class="markdownIt-Anchor" href="#论文背景"></a> 论文背景</h3>
<p>传统的协同过滤主要学习两点。</p>
<ol>
<li>学习物品和用户的表示。</li>
<li>学习物品和用户的交互。<br />
但是它们都缺乏显式的协同信号。解决这两个问题的办法就是探究高阶的连接交互。虽然近期Hop-Rec也提出类似的概念，但是仅仅丰富了训练数据，最后交互依然用的MF。所以它们是本质不同的。</li>
</ol>
<h3 id="模型结构"><a class="markdownIt-Anchor" href="#模型结构"></a> 模型结构</h3>
<p>模型结构比较容易。</p>
<img src="/2020/06/11/Neural-Graph-Collaborate-Filtering/1.png" class="">
<h4 id="user-embeddings-and-item-embeddings"><a class="markdownIt-Anchor" href="#user-embeddings-and-item-embeddings"></a> user embeddings and item embeddings</h4>
<img src="/2020/06/11/Neural-Graph-Collaborate-Filtering/2.png" class="">
<h4 id="模型结构图"><a class="markdownIt-Anchor" href="#模型结构图"></a> 模型结构图</h4>
<img src="/2020/06/11/Neural-Graph-Collaborate-Filtering/3.png" class="">
<img src="/2020/06/11/Neural-Graph-Collaborate-Filtering/4.png" class="">
<h4 id="first-order-propagation"><a class="markdownIt-Anchor" href="#first-order-propagation"></a> First order propagation</h4>
<p>$ m_{u \leftarrow i} = f(e_i,e_u,p_{ui}) $</p>
<p>其中文中给的$ f $:</p>
<p>$ m_{u \leftarrow i} = \frac{1}{\sqrt(|N_u| )\sqrt(|N_i| )}(W_1e_i+W_2(e_i \odot e_u)) $</p>
<h4 id="message-aggregation"><a class="markdownIt-Anchor" href="#message-aggregation"></a> Message Aggregation</h4>
<p>$ e_u^{(1)} = LeakyRelu(m_{u \leftarrow u} + \Sigma_{i \in N_u} m_{u \leftarrow i}) $</p>
<h4 id="higher-order-propagation"><a class="markdownIt-Anchor" href="#higher-order-propagation"></a> Higher order propagation</h4>
<p>和First order propagation类似</p>
<h4 id="model-prediction"><a class="markdownIt-Anchor" href="#model-prediction"></a> Model prediction</h4>
<p>$ y_{NGCF}(u,i) = e_u ^T e_i $</p>
<h4 id="optimization"><a class="markdownIt-Anchor" href="#optimization"></a> Optimization</h4>
<p>$ Loss = \Sigma_{(u,i,j) \in O } -ln \sigma(\hat{y_{u,i}} - \hat{y_{u,j}} ) + \lambda || \theta||^2_2 $</p>
<h4 id="model-size"><a class="markdownIt-Anchor" href="#model-size"></a> Model size</h4>
<p>用了很少的额外空间就达到了高阶的连接。</p>
<h4 id="message-and-node-dropout"><a class="markdownIt-Anchor" href="#message-and-node-dropout"></a> Message and Node dropout</h4>
<p>node dropout能够提高泛化能力。</p>
<h3 id="实验效果"><a class="markdownIt-Anchor" href="#实验效果"></a> 实验效果</h3>
<img src="/2020/06/11/Neural-Graph-Collaborate-Filtering/5.png" class="">
<img src="/2020/06/11/Neural-Graph-Collaborate-Filtering/6.png" class="">
<p>总的来看,实验结果还是挺不错的。但是依据历史发展规律来看，还有很多可以改进。不然也不会出lightgcn了哈哈。</p>
<h3 id="代码"><a class="markdownIt-Anchor" href="#代码"></a> 代码</h3>
<p><a href="https://github.com/talkingwallace/NGCF-pytorch/tree/master/GraphNCF" target="_blank" rel="noopener">Github</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.nn import Module</span><br><span class="line">from scipy.sparse import coo_matrix</span><br><span class="line">from scipy.sparse import vstack</span><br><span class="line">from scipy import sparse</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class SVD(Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self,userNum,itemNum,dim):</span><br><span class="line">        super(SVD, self).__init__()</span><br><span class="line">        self.uEmbd &#x3D; nn.Embedding(userNum,dim)</span><br><span class="line">        self.iEmbd &#x3D; nn.Embedding(itemNum,dim)</span><br><span class="line">        self.uBias &#x3D; nn.Embedding(userNum,1)</span><br><span class="line">        self.iBias &#x3D; nn.Embedding(itemNum,1)</span><br><span class="line">        self.overAllBias &#x3D; nn.Parameter(torch.Tensor([0]))</span><br><span class="line"></span><br><span class="line">    def forward(self, userIdx,itemIdx):</span><br><span class="line">        uembd &#x3D; self.uEmbd(userIdx)</span><br><span class="line">        iembd &#x3D; self.iEmbd(itemIdx)</span><br><span class="line">        ubias &#x3D; self.uBias(userIdx)</span><br><span class="line">        ibias &#x3D; self.iBias(itemIdx)</span><br><span class="line"></span><br><span class="line">        biases &#x3D; ubias + ibias + self.overAllBias</span><br><span class="line">        prediction &#x3D; torch.sum(torch.mul(uembd,iembd),dim&#x3D;1) + biases.flatten()</span><br><span class="line"></span><br><span class="line">        return prediction</span><br><span class="line"></span><br><span class="line">class NCF(Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self,userNum,itemNum,dim,layers&#x3D;[128,64,32,8]):</span><br><span class="line">        super(NCF, self).__init__()</span><br><span class="line">        self.uEmbd &#x3D; nn.Embedding(userNum,dim)</span><br><span class="line">        self.iEmbd &#x3D; nn.Embedding(itemNum,dim)</span><br><span class="line">        self.fc_layers &#x3D; torch.nn.ModuleList()</span><br><span class="line">        self.finalLayer &#x3D; torch.nn.Linear(layers[-1],1)</span><br><span class="line"></span><br><span class="line">        for From,To in zip(layers[:-1],layers[1:]):</span><br><span class="line">            self.fc_layers.append(nn.Linear(From,To))</span><br><span class="line"></span><br><span class="line">    def forward(self, userIdx,itemIdx):</span><br><span class="line">        uembd &#x3D; self.uEmbd(userIdx)</span><br><span class="line">        iembd &#x3D; self.iEmbd(itemIdx)</span><br><span class="line">        embd &#x3D; torch.cat([uembd, iembd], dim&#x3D;1)</span><br><span class="line">        x &#x3D; embd</span><br><span class="line">        for l in self.fc_layers:</span><br><span class="line">            x &#x3D; l(x)</span><br><span class="line">            x &#x3D; nn.ReLU()(x)</span><br><span class="line"></span><br><span class="line">        prediction &#x3D; self.finalLayer(x)</span><br><span class="line">        return prediction.flatten()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class GNNLayer(Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self,inF,outF):</span><br><span class="line"></span><br><span class="line">        super(GNNLayer,self).__init__()</span><br><span class="line">        self.inF &#x3D; inF</span><br><span class="line">        self.outF &#x3D; outF</span><br><span class="line">        self.linear &#x3D; torch.nn.Linear(in_features&#x3D;inF,out_features&#x3D;outF)</span><br><span class="line">        self.interActTransform &#x3D; torch.nn.Linear(in_features&#x3D;inF,out_features&#x3D;outF)</span><br><span class="line"></span><br><span class="line">    def forward(self, laplacianMat,selfLoop,features):</span><br><span class="line">        # for GCF ajdMat is a (N+M) by (N+M) mat</span><br><span class="line">        # laplacianMat L &#x3D; D^-1(A)D^-1 # 拉普拉斯矩阵</span><br><span class="line">        L1 &#x3D; laplacianMat + selfLoop</span><br><span class="line">        L2 &#x3D; laplacianMat.cuda()</span><br><span class="line">        L1 &#x3D; L1.cuda()</span><br><span class="line">        inter_feature &#x3D; torch.sparse.mm(L2,features)</span><br><span class="line">        inter_feature &#x3D; torch.mul(inter_feature,features)</span><br><span class="line"></span><br><span class="line">        inter_part1 &#x3D; self.linear(torch.sparse.mm(L1,features))</span><br><span class="line">        inter_part2 &#x3D; self.interActTransform(torch.sparse.mm(L2,inter_feature))</span><br><span class="line"></span><br><span class="line">        return inter_part1+inter_part2</span><br><span class="line"></span><br><span class="line">class GCF(Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self,userNum,itemNum,rt,embedSize&#x3D;100,layers&#x3D;[100,80,50],useCuda&#x3D;True):</span><br><span class="line"></span><br><span class="line">        super(GCF,self).__init__()</span><br><span class="line">        self.useCuda &#x3D; useCuda</span><br><span class="line">        self.userNum &#x3D; userNum</span><br><span class="line">        self.itemNum &#x3D; itemNum</span><br><span class="line">        self.uEmbd &#x3D; nn.Embedding(userNum,embedSize)</span><br><span class="line">        self.iEmbd &#x3D; nn.Embedding(itemNum,embedSize)</span><br><span class="line">        self.GNNlayers &#x3D; torch.nn.ModuleList()</span><br><span class="line">        self.LaplacianMat &#x3D; self.buildLaplacianMat(rt) # sparse format</span><br><span class="line">        self.leakyRelu &#x3D; nn.LeakyReLU()</span><br><span class="line">        self.selfLoop &#x3D; self.getSparseEye(self.userNum+self.itemNum)</span><br><span class="line"></span><br><span class="line">        self.transForm1 &#x3D; nn.Linear(in_features&#x3D;layers[-1]*(len(layers))*2,out_features&#x3D;64)</span><br><span class="line">        self.transForm2 &#x3D; nn.Linear(in_features&#x3D;64,out_features&#x3D;32)</span><br><span class="line">        self.transForm3 &#x3D; nn.Linear(in_features&#x3D;32,out_features&#x3D;1)</span><br><span class="line"></span><br><span class="line">        for From,To in zip(layers[:-1],layers[1:]):</span><br><span class="line">            self.GNNlayers.append(GNNLayer(From,To))</span><br><span class="line"></span><br><span class="line">    def getSparseEye(self,num):</span><br><span class="line">        i &#x3D; torch.LongTensor([[k for k in range(0,num)],[j for j in range(0,num)]])</span><br><span class="line">        val &#x3D; torch.FloatTensor([1]*num)</span><br><span class="line">        return torch.sparse.FloatTensor(i,val)</span><br><span class="line"></span><br><span class="line">    def buildLaplacianMat(self,rt):</span><br><span class="line"></span><br><span class="line">        rt_item &#x3D; rt[&#39;itemId&#39;] + self.userNum</span><br><span class="line">        uiMat &#x3D; coo_matrix((rt[&#39;rating&#39;], (rt[&#39;userId&#39;], rt[&#39;itemId&#39;])))</span><br><span class="line"></span><br><span class="line">        uiMat_upperPart &#x3D; coo_matrix((rt[&#39;rating&#39;], (rt[&#39;userId&#39;], rt_item)))</span><br><span class="line">        uiMat &#x3D; uiMat.transpose()</span><br><span class="line">        uiMat.resize((self.itemNum, self.userNum + self.itemNum))</span><br><span class="line"></span><br><span class="line">        A &#x3D; sparse.vstack([uiMat_upperPart,uiMat])</span><br><span class="line">        selfLoop &#x3D; sparse.eye(self.userNum+self.itemNum)</span><br><span class="line">        sumArr &#x3D; (A&gt;0).sum(axis&#x3D;1)</span><br><span class="line">        diag &#x3D; list(np.array(sumArr.flatten())[0])</span><br><span class="line">        diag &#x3D; np.power(diag,-0.5)</span><br><span class="line">        D &#x3D; sparse.diags(diag)</span><br><span class="line">        L &#x3D; D * A * D</span><br><span class="line">        L &#x3D; sparse.coo_matrix(L)</span><br><span class="line">        row &#x3D; L.row</span><br><span class="line">        col &#x3D; L.col</span><br><span class="line">        i &#x3D; torch.LongTensor([row,col])</span><br><span class="line">        data &#x3D; torch.FloatTensor(L.data)</span><br><span class="line">        SparseL &#x3D; torch.sparse.FloatTensor(i,data)</span><br><span class="line">        return SparseL</span><br><span class="line"></span><br><span class="line">    def getFeatureMat(self):</span><br><span class="line">        uidx &#x3D; torch.LongTensor([i for i in range(self.userNum)])</span><br><span class="line">        iidx &#x3D; torch.LongTensor([i for i in range(self.itemNum)])</span><br><span class="line">        if self.useCuda &#x3D;&#x3D; True:</span><br><span class="line">            uidx &#x3D; uidx.cuda()</span><br><span class="line">            iidx &#x3D; iidx.cuda()</span><br><span class="line"></span><br><span class="line">        userEmbd &#x3D; self.uEmbd(uidx)</span><br><span class="line">        itemEmbd &#x3D; self.iEmbd(iidx)</span><br><span class="line">        features &#x3D; torch.cat([userEmbd,itemEmbd],dim&#x3D;0)</span><br><span class="line">        return features</span><br><span class="line"></span><br><span class="line">    def forward(self,userIdx,itemIdx):</span><br><span class="line"></span><br><span class="line">        itemIdx &#x3D; itemIdx + self.userNum</span><br><span class="line">        userIdx &#x3D; list(userIdx.cpu().data)</span><br><span class="line">        itemIdx &#x3D; list(itemIdx.cpu().data)</span><br><span class="line">        # gcf data propagation</span><br><span class="line">        features &#x3D; self.getFeatureMat()</span><br><span class="line">        finalEmbd &#x3D; features.clone()</span><br><span class="line">        for gnn in self.GNNlayers:</span><br><span class="line">            features &#x3D; gnn(self.LaplacianMat,self.selfLoop,features)</span><br><span class="line">            features &#x3D; nn.ReLU()(features)</span><br><span class="line">            finalEmbd &#x3D; torch.cat([finalEmbd,features.clone()],dim&#x3D;1)</span><br><span class="line"></span><br><span class="line">        userEmbd &#x3D; finalEmbd[userIdx]</span><br><span class="line">        itemEmbd &#x3D; finalEmbd[itemIdx]</span><br><span class="line">        embd &#x3D; torch.cat([userEmbd,itemEmbd],dim&#x3D;1)</span><br><span class="line"></span><br><span class="line">        embd &#x3D; nn.ReLU()(self.transForm1(embd))</span><br><span class="line">        embd &#x3D; self.transForm2(embd)</span><br><span class="line">        embd &#x3D; self.transForm3(embd)</span><br><span class="line">        prediction &#x3D; embd.flatten()</span><br><span class="line"></span><br><span class="line">        return prediction</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://jaykay233.github.io/2020/06/11/Search-based-User-Interest-Modeling-with-Lifelong-Sequential/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jay Kay">
      <meta itemprop="description" content="Zhiyuan Xu's self blog!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="徐志远">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/11/Search-based-User-Interest-Modeling-with-Lifelong-Sequential/" class="post-title-link" itemprop="url">Search-based User Interest Modeling with Lifelong Sequential</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-11 15:23:15" itemprop="dateCreated datePublished" datetime="2020-06-11T15:23:15-07:00">2020-06-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://jaykay233.github.io/2020/06/11/phased-lstm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jay Kay">
      <meta itemprop="description" content="Zhiyuan Xu's self blog!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="徐志远">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/11/phased-lstm/" class="post-title-link" itemprop="url">Time-related LSTM</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-11 10:43:39" itemprop="dateCreated datePublished" datetime="2020-06-11T10:43:39-07:00">2020-06-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">神经网络</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>其实之前有面试被问到过类似怎么融合时间的问题。当时见的还比较少，直接说了根据时间抽样，现在看来正确答案貌似应该是这个。</p>
<h3 id="论文意图和背景"><a class="markdownIt-Anchor" href="#论文意图和背景"></a> 论文意图和背景</h3>
<p>长话短说，其实就是要在lstm中加入时间的概念</p>
<h3 id="传统的lstm"><a class="markdownIt-Anchor" href="#传统的lstm"></a> 传统的LSTM</h3>
<p>$ i_t = \sigma_{i}(x_tW_{xi} + h_{t-1}W_{hi} + w_{ci} \odot c_{t-1} + b_i ) $</p>
<p>$ f_t = \sigma_{f}(x_tW_{xf} + h_{t-1}W_{hf} + w_{cf} \odot c_{t-1} + b_f) $</p>
<p>$ c_t = f_t \odot c_{t-1} + i_t \odot \sigma_{c}(x_tW_{xc} + h_{t-1}W_{hc} + b_c) $</p>
<p>$ o_t = \sigma_{o}(x_tW_{xo} + h_{t-1}W_{ho} + w_{co} \odot c_t + b_o ) $</p>
<p>$ h_t = o_t \odot \sigma_{h}(c_t) $</p>
<h3 id="phased-lstm"><a class="markdownIt-Anchor" href="#phased-lstm"></a> Phased LSTM</h3>
<p>Phased LSTM增加了time gate。开启和关闭这个门由三个参数决定。<br />
$ c_t $和 $ h_t $ 仅仅在门开启的时候允许通过。$ \tau $ 控制了震荡的时间。$ \tau_{on} $ 控制了开相对于整个时间周期的比例。<br />
$ s $  控制了相位的偏移。这些参数都是可以学习的。<br />
$ \phi_{t} = \frac{(t-s) mod \tau }{ \tau } $</p>
<img src="/images/phased_lstm/phased_lstm_1.png" width="30%" height="30%">
所以phased_lstm的公式还需要time gate的更新：
<img src="/images/phased_lstm/phased_lstm_2.png" width="50%" height="50%">
Phased LSTM还有一个好处, 它能更好地保持初始信息。
传统的lstm:
<p>$ c_n = f_n \odot c_{n-1} = (1-\epsilon) \odot (f_{n-1} \odot c_{n-2} ) = … = (1-\epsilon)^n \odot c_0 $</p>
<p>相反地, 在gate关闭的时候, Phased LSTM可以很好地保存信息。</p>
<h3 id="phased-lstm实验结果"><a class="markdownIt-Anchor" href="#phased-lstm实验结果"></a> Phased LSTM实验结果</h3>
<img src="/images/phased_lstm/phased_lstm_3.png" width="100%" height="100%">
可以看到在一些数据集上其实效果远胜传统的lstm,具体可查看原论文。
<h3 id="phased-lstm缺点"><a class="markdownIt-Anchor" href="#phased-lstm缺点"></a> Phased LSTM缺点</h3>
<p>当然, Phased LSTM也有一些缺点。</p>
<ol>
<li>它仅仅考虑时间点的建模，而没有考虑时间间隔建模。在推荐系统中，Phased LSTM仅仅对考虑了用户的活动状态而未考虑用户的静止状态(因为Phased LSTM设置了静止状态)。</li>
<li>并不能区分短时兴趣和长期兴趣的影响。</li>
</ol>
<h3 id="time-lstm"><a class="markdownIt-Anchor" href="#time-lstm"></a> Time LSTM</h3>
<p>Time LSTM的提出就是为了克服Phased LSTM的问题。给定:</p>
<img src="/images/phased_lstm/phased_lstm_4.png" width="50%" height="50%">
实际建模时会把单点的时间转换为时间间隔建模。
<h4 id="time-lstm变体"><a class="markdownIt-Anchor" href="#time-lstm变体"></a> Time LSTM变体</h4>
<h5 id="time-lstm变体1"><a class="markdownIt-Anchor" href="#time-lstm变体1"></a> Time LSTM变体1</h5>
<img src="/images/phased_lstm/phased_lstm_5.png" width="50%" height="50%">
<img src="/images/phased_lstm/phased_lstm_6.png" width="50%" height="50%">
这是Phased LSTM最简单的改变,仅仅是把时间点换成了时间间隔,非常清晰易懂。
<h5 id="time-lstm变体2"><a class="markdownIt-Anchor" href="#time-lstm变体2"></a> Time LSTM变体2</h5>
<img src="/images/phased_lstm/phased_lstm_7.png" width="50%" height="50%">
<img src="/images/phased_lstm/phased_lstm_8.png" width="50%" height="50%">
使用了2个Time gate。Time gate1主要用来利用现在的时间间隔来进行现在物品推荐,Time gate2主要用来存储时间间隔为以后的推荐准备。
<h5 id="time-lstm变体3"><a class="markdownIt-Anchor" href="#time-lstm变体3"></a> Time LSTM变体3</h5>
<img src="/images/phased_lstm/phased_lstm_9.png" width="50%" height="50%">
<img src="/images/phased_lstm/phased_lstm_10.png" width="50%" height="50%">
Time LSTM变体3用了耦合的输入门和忘记门。
<h3 id="time-lstm实验结果"><a class="markdownIt-Anchor" href="#time-lstm实验结果"></a> Time LSTM实验结果</h3>
<img src="/images/phased_lstm/phased_lstm_11.png" width="50%" height="50%">
可以看到Time LSTM确实在效果上优于之前的Time-related LSTM方法
<h3 id="phased-lstm-torch开源实现"><a class="markdownIt-Anchor" href="#phased-lstm-torch开源实现"></a> Phased LSTM torch开源实现</h3>
<p><a href="https://gist.github.com/AAnoosheh/b3cce037ae24a86d89505907ec098d62" target="_blank" rel="noopener">github地址</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line">import math</span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class PhasedLSTMCell(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;Phased LSTM recurrent network cell.</span><br><span class="line">    https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1610.09513v1.pdf</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(</span><br><span class="line">        self,</span><br><span class="line">        hidden_size,</span><br><span class="line">        leak&#x3D;0.001,</span><br><span class="line">        ratio_on&#x3D;0.1,</span><br><span class="line">        period_init_min&#x3D;1.0,</span><br><span class="line">        period_init_max&#x3D;1000.0</span><br><span class="line">    ):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Args:</span><br><span class="line">            hidden_size: int, The number of units in the Phased LSTM cell.</span><br><span class="line">            leak: float or scalar float Tensor with value in [0, 1]. Leak applied</span><br><span class="line">                during training.</span><br><span class="line">            ratio_on: float or scalar float Tensor with value in [0, 1]. Ratio of the</span><br><span class="line">                period during which the gates are open.</span><br><span class="line">            period_init_min: float or scalar float Tensor. With value &gt; 0.</span><br><span class="line">                Minimum value of the initialized period.</span><br><span class="line">                The period values are initialized by drawing from the distribution:</span><br><span class="line">                e^U(log(period_init_min), log(period_init_max))</span><br><span class="line">                Where U(.,.) is the uniform distribution.</span><br><span class="line">            period_init_max: float or scalar float Tensor.</span><br><span class="line">                With value &gt; period_init_min. Maximum value of the initialized period.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.hidden_size &#x3D; hidden_size</span><br><span class="line">        self.ratio_on &#x3D; ratio_on</span><br><span class="line">        self.leak &#x3D; leak</span><br><span class="line"></span><br><span class="line">        # initialize time-gating parameters</span><br><span class="line">        period &#x3D; torch.exp(</span><br><span class="line">            torch.Tensor(hidden_size).uniform_(</span><br><span class="line">                math.log(period_init_min), math.log(period_init_max)</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        self.tau &#x3D; nn.Parameter(period)</span><br><span class="line"></span><br><span class="line">        phase &#x3D; torch.Tensor(hidden_size).uniform_() * period</span><br><span class="line">        self.phase &#x3D; nn.Parameter(phase)</span><br><span class="line"></span><br><span class="line">    def _compute_phi(self, t):</span><br><span class="line">        t_ &#x3D; t.view(-1, 1).repeat(1, self.hidden_size)</span><br><span class="line">        phase_ &#x3D; self.phase.view(1, -1).repeat(t.shape[0], 1)</span><br><span class="line">        tau_ &#x3D; self.tau.view(1, -1).repeat(t.shape[0], 1)</span><br><span class="line"></span><br><span class="line">        phi &#x3D; torch.fmod((t_ - phase_), tau_).detach()</span><br><span class="line">        phi &#x3D; torch.abs(phi) &#x2F; tau_</span><br><span class="line">        return phi</span><br><span class="line"></span><br><span class="line">    def _mod(self, x, y):</span><br><span class="line">        &quot;&quot;&quot;Modulo function that propagates x gradients.&quot;&quot;&quot;</span><br><span class="line">        return x + (torch.fmod(x, y) - x).detach()</span><br><span class="line"></span><br><span class="line">    def set_state(self, c, h):</span><br><span class="line">        self.h0 &#x3D; h</span><br><span class="line">        self.c0 &#x3D; c</span><br><span class="line"></span><br><span class="line">    def forward(self, c_s, h_s, t):</span><br><span class="line">        # print(c_s.size(), h_s.size(), t.size())</span><br><span class="line">        phi &#x3D; self._compute_phi(t)</span><br><span class="line"></span><br><span class="line">        # Phase-related augmentations</span><br><span class="line">        k_up &#x3D; 2 * phi &#x2F; self.ratio_on</span><br><span class="line">        k_down &#x3D; 2 - k_up</span><br><span class="line">        k_closed &#x3D; self.leak * phi</span><br><span class="line"></span><br><span class="line">        k &#x3D; torch.where(phi &lt; self.ratio_on, k_down, k_closed)</span><br><span class="line">        k &#x3D; torch.where(phi &lt; 0.5 * self.ratio_on, k_up, k)</span><br><span class="line">        k &#x3D; k.view(c_s.shape[0], t.shape[0], -1)</span><br><span class="line"></span><br><span class="line">        c_s_new &#x3D; k * c_s + (1 - k) * self.c0</span><br><span class="line">        h_s_new &#x3D; k * h_s + (1 - k) * self.h0</span><br><span class="line"></span><br><span class="line">        return h_s_new, c_s_new</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class PhasedLSTM(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;Wrapper for multi-layer sequence forwarding via</span><br><span class="line">       PhasedLSTMCell&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(</span><br><span class="line">        self,</span><br><span class="line">        input_size,</span><br><span class="line">        hidden_size,</span><br><span class="line">        bidirectional&#x3D;True</span><br><span class="line">    ):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.hidden_size &#x3D; hidden_size</span><br><span class="line"></span><br><span class="line">        self.lstm &#x3D; nn.LSTM(</span><br><span class="line">            input_size&#x3D;input_size,</span><br><span class="line">            hidden_size&#x3D;hidden_size,</span><br><span class="line">            bidirectional&#x3D;bidirectional,</span><br><span class="line">            batch_first&#x3D;True</span><br><span class="line">        )</span><br><span class="line">        self.bi &#x3D; 2 if bidirectional else 1</span><br><span class="line"></span><br><span class="line">        self.phased_cell &#x3D; PhasedLSTMCell(</span><br><span class="line">            hidden_size&#x3D;self.bi * hidden_size</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, u_sequence):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Args:</span><br><span class="line">            sequence: The input sequence data of shape (batch, time, N)</span><br><span class="line">            times: The timestamps corresponding to the data of shape (batch, time)</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        c0 &#x3D; u_sequence.new_zeros((self.bi, u_sequence.size(0), self.hidden_size))</span><br><span class="line">        h0 &#x3D; u_sequence.new_zeros((self.bi, u_sequence.size(0), self.hidden_size))</span><br><span class="line">        self.phased_cell.set_state(c0, h0)</span><br><span class="line"></span><br><span class="line">        outputs &#x3D; []</span><br><span class="line">        for i in range(u_sequence.size(1)):</span><br><span class="line">            u_t &#x3D; u_sequence[:, i, :-1].unsqueeze(1)</span><br><span class="line">            t_t &#x3D; u_sequence[:, i, -1]</span><br><span class="line"></span><br><span class="line">            out, (c_t, h_t) &#x3D; self.lstm(u_t, (c0, h0))</span><br><span class="line">            (c_s, h_s) &#x3D; self.phased_cell(c_t, h_t, t_t)</span><br><span class="line"></span><br><span class="line">            self.phased_cell.set_state(c_s, h_s)</span><br><span class="line">            c0, h0 &#x3D; c_s, h_s</span><br><span class="line"></span><br><span class="line">            outputs.append(out)</span><br><span class="line">        outputs &#x3D; torch.cat(outputs, dim&#x3D;1)</span><br><span class="line"></span><br><span class="line">        return outputs</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://jaykay233.github.io/2020/06/10/Content-aware-Neural-Hashing-for-Cold-start-Recommendation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jay Kay">
      <meta itemprop="description" content="Zhiyuan Xu's self blog!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="徐志远">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/10/Content-aware-Neural-Hashing-for-Cold-start-Recommendation/" class="post-title-link" itemprop="url">Content-aware Neural Hashing for Cold-start Recommendation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-10 20:02:59" itemprop="dateCreated datePublished" datetime="2020-06-10T20:02:59-07:00">2020-06-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">推荐系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这是一篇sigir2020推荐系统的文章。</p>
<h3 id="论文背景"><a class="markdownIt-Anchor" href="#论文背景"></a> 论文背景：</h3>
<p>通过学习item的hashcode表示来进行冷启动。基于协同过滤和内容的推荐系统是很主流的方法。</p>
<h4 id="协同过滤的缺点"><a class="markdownIt-Anchor" href="#协同过滤的缺点"></a> 协同过滤的缺点：</h4>
<p>对于未见过的item，标准的协同过滤不能学习有效表示。所以基于内容的推荐系统可以补充标准协同过滤的方法。</p>
<h4 id="基于内容的推荐系统的缺点"><a class="markdownIt-Anchor" href="#基于内容的推荐系统的缺点"></a> 基于内容的推荐系统的缺点</h4>
<p>基于内容(content-aware)hashcode表示个生成和标准item的生成不同,是不必要的并且会限制其泛化性能。</p>
<h3 id="neuhash-cf"><a class="markdownIt-Anchor" href="#neuhash-cf"></a> NeuHash-CF</h3>
<p><img src="/images/neuhash-cf/neuhash-cf.png" alt="" /><br />
NeuHash-CF有两个encoder来生成item和user的hashcode, 它们连接在变分自编码模型中. 所有的item code都统一生成,不管是否曾经见过。</p>
<h3 id="variational-autoencoder"><a class="markdownIt-Anchor" href="#variational-autoencoder"></a> Variational AutoEncoder</h3>
<p>$ p(u) = \prod_{i \in I_u} p(R_{u,i}) $</p>
<p>$ p(i) = p(c_i) + \prod_{u \in U_i } {p(R_{u,i})} $</p>
<p>$ p(c_i) = \prod_{w\in W_{c_i} } p(w) $</p>
<p>$ logp(R_{u,i}) = log \Sigma_{z_i,z_u \in \{ -1,1 \}^m }p(R_{u,i}|z_i,z_u)p(z_i)p(z_u) $</p>
<p>$ logp(c_i) = log \Sigma_{z_i \in \{ -1,1\}^m } p(c_i |z_i)p(z_i) $</p>
<p>上面的公式把content-based推荐和cf推荐进行联合建模。后续的hashcode采样和em优化详细可以看文章。</p>
<h3 id="encoder-function"><a class="markdownIt-Anchor" href="#encoder-function"></a> Encoder Function</h3>
<p>item encoding:<br />
$ l_1 = ReLU(W_1(c_i \odot w_{imp} ) + b_1) $</p>
<p>$ l_2 = ReLU(W_2l_1 + b_2) $</p>
<p>item采样概率: $ q_{\phi}(c_i) = \sigma_{(W_3l_3+b_3)} $</p>
<p>user和item类似的情况。</p>
<!-- $ z_i^{(j)} = 2\lceil{q_{\phi}(i)^{(j)}-\mu^{(j)}}\rceil-1 $ -->
<h3 id="decoder-function"><a class="markdownIt-Anchor" href="#decoder-function"></a> Decoder Function</h3>
<p>User-item rating decoding<br />
Item content decoding<br />
Noise infusion for robustness</p>
<h3 id="combined-loss-function"><a class="markdownIt-Anchor" href="#combined-loss-function"></a> Combined loss function</h3>
<p>$ L = L_{rating} + \alpha L_{content} $</p>
<h3 id="实验结果"><a class="markdownIt-Anchor" href="#实验结果"></a> 实验结果</h3>
<p><img src="/images/neuhash-cf/evaluation1.png" alt="" /><br />
<img src="/images/neuhash-cf/evaluation2.png" alt="" /><br />
看实验效果感觉还可以。就是建模比较复杂，期待源码。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jay Kay</p>
  <div class="site-description" itemprop="description">Zhiyuan Xu's self blog!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.github.com/jaykay233" title="http:&#x2F;&#x2F;www.github.com&#x2F;jaykay233" rel="noopener" target="_blank">GitHub</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jay Kay</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>










<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'http://jaykay233.github.io/',]
      });
      });
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
